{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please notice that the Telia data which is used as an input to this script is private and thus not provided in the library. Neverteless, the script is provided in case the reader eants to reproduce this study on a simillar dataset of their own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stat\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the repository we are working in so later is easier to read/write files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.abspath('')\n",
    "data_path = os.path.join(dir_path, 'data')\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_weekday(day, month, year):\n",
    "    parsed_date = dt.date(int(year), int(month), int(day))\n",
    "    weekday_map = {0:\"Monday\", 1:\"Tuesday\", 2:\"Wednesday\", 3:\"Thursday\", 4:\"Friday\", 5:\"Saturday\", 6:\"Sunday\"}\n",
    "    return((weekday_map[parsed_date.weekday()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will read the file kunta_utf-8_trimmed_include_both_swedish_and_Finnish.csv and generate 2 dictionaries. The first one will map the hospital care districts (HCD) to the municipalities. The second one will do the inverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_municiplaities_to_hospital_district_areas(data_path):\n",
    "    #This function returns two dictionaries h_to_m and m_to_h.\n",
    "    #h_to_m keys are HCDs and the values are list of their municipalities.\n",
    "    #m_to_h keys are municipalities and their corresponding values the HCDs that they belong to.\n",
    "    line_counter = 0\n",
    "    m_to_h = {}\n",
    "    h_to_m = {}\n",
    "    with open(data_path, 'r') as data:\n",
    "        for line in data:\n",
    "            line_counter += 1\n",
    "            ### the enteries start from 6th line\n",
    "            if line_counter > 5:\n",
    "                fields = line.strip().split(';')\n",
    "                municipality_string = fields[1]\n",
    "                #print(municipality_string)\n",
    "                municipality = municipality_string[1:-1]\n",
    "                #print(municipality)\n",
    "                hospital_string = fields[3]\n",
    "                hospital_district = hospital_string[1:-1]\n",
    "                #print(hospital_district)\n",
    "                m_to_h[municipality] = hospital_district\n",
    "                if hospital_district not in h_to_m.keys():\n",
    "                    h_to_m[hospital_district] = set()\n",
    "                h_to_m[hospital_district].add(municipality)\n",
    "    return h_to_m, m_to_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_to_h_data_converted_format = os.path.join(data_path, 'kunta_utf-8_trimmed_include_both_swedish_and_Finnish.csv')\n",
    "h_to_m, m_to_h = map_municiplaities_to_hospital_district_areas(m_to_h_data_converted_format)\n",
    "#print(m_to_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_hcd_out_in_telia(telia_datapath, municipality_to_hcd_map, out_degree_save_path = None, in_degree_save_path = None):\n",
    "    hcd_in_degree = {} # a dictionary with (destination, hour, day, month, year) as key and total in-degree as value\n",
    "    hcd_out_degree = {} # a dictionary with (origin, hour, day, month, year) as key and total out-degree as value\n",
    "    with open(telia_datapath, 'r') as data:\n",
    "        next(data)\n",
    "        for line in data:\n",
    "            #print(line)\n",
    "            fields = line.strip().split(\",\")\n",
    "            origin_m = fields[3]\n",
    "            destination_m = fields[7]\n",
    "            origin_hcd = municipality_to_hcd_map[origin_m]\n",
    "            destination_hcd = municipality_to_hcd_map[destination_m]\n",
    "            if origin_hcd != destination_hcd:\n",
    "                date_string = fields[1]\n",
    "                date_string_splitted = (date_string).split(\"-\")\n",
    "                year = str(int(date_string_splitted[0]))\n",
    "                month = str(int(date_string_splitted[1]))\n",
    "                day = str(int(date_string_splitted[2]))\n",
    "                hour = str(int(fields[-1].split(\"-\")[0]))\n",
    "                flow = int(fields[0])\n",
    "                if (destination_hcd, hour, day, month, year) not in hcd_in_degree.keys():\n",
    "                    hcd_in_degree[(destination_hcd, hour, day, month, year)] = 0\n",
    "                hcd_in_degree[(destination_hcd, hour, day, month, year)] += flow \n",
    "                if (origin_hcd, hour, day, month, year) not in hcd_out_degree.keys():\n",
    "                    hcd_out_degree[(origin_hcd, hour, day, month, year)] = 0\n",
    "                hcd_out_degree[(origin_hcd, hour, day, month, year)] += flow\n",
    "            \n",
    "    ### print the in degree and out degree to files\n",
    "    if out_degree_save_path is not None:\n",
    "        out_key_list = list(hcd_out_degree.keys())\n",
    "        sorted_out_key_list = sorted(out_key_list,key=lambda x: (x[0], x[4], x[3], x[2], x[1]))\n",
    "        with open(out_degree_save_path, \"w\") as out_save_file:\n",
    "            header = \"in-or-out, hcd, hour, day, month, year, flow\"\n",
    "            out_save_file.write(header+\"\\n\")\n",
    "            for key in sorted_out_key_list:\n",
    "                out_save_file.write(\"outdegree,\"+','.join([str(key[0]),str(key[1]),str(key[2]),str(key[3]),str(key[4])])+','+str(hcd_out_degree[key])+\"\\n\")\n",
    "    if in_degree_save_path is not None:\n",
    "        in_key_list = list(hcd_in_degree.keys())\n",
    "        sorted_in_key_list = sorted(in_key_list,key=lambda x: (x[0], x[4], x[3], x[2], x[1]))\n",
    "        with open(in_degree_save_path, \"w\") as in_save_file:\n",
    "            header = \"in-or-out, hcd, hour, day, month, year, flow\"\n",
    "            in_save_file.write(header+\"\\n\")\n",
    "            for key in sorted_in_key_list:\n",
    "                in_save_file.write(\"indegree,\"+','.join([str(key[0]),str(key[1]),str(key[2]),str(key[3]),str(key[4])])+','+str(hcd_in_degree[key])+\"\\n\")\n",
    "    return(hcd_in_degree, hcd_out_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "telia_datapath = '/.../GSE_od_muni_concatenated_aland_and_days_missing_more_than_10_municipalities_removed.csv'\n",
    "\n",
    "hcd_telia_indegree_book, hcd_telia_outdegree_book = calculate_total_hcd_out_in_telia(telia_datapath, m_to_h, None, None)\n",
    "hcd_telia_indegree_book_2019 = {key:value for (key, value) in hcd_telia_indegree_book.items() if key[4] == \"2019\"}\n",
    "hcd_telia_outdegree_book_2019 = {key:value for (key, value) in hcd_telia_outdegree_book.items() if key[4] == \"2019\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def telia_od_book_hcd_level(telia_data_path, municipality_to_hcd_map):\n",
    "    #trips_sum,date,origin_muni_code,origin_muni,origin_region_code,origin_region,dest_muni_code,dest_muni,dest_region_code,dest_region,hour_bin\n",
    "    od_book = {}\n",
    "    with open(telia_data_path, 'r') as data:\n",
    "        next(data)\n",
    "        for line in data:\n",
    "            fields = line.strip().split(\",\")\n",
    "            origin_m = fields[3]\n",
    "            destination_m = fields[7]\n",
    "            origin_hcd = municipality_to_hcd_map[origin_m]\n",
    "            destination_hcd = municipality_to_hcd_map[destination_m]\n",
    "            date_string = fields[1]\n",
    "            date_string_splitted = (date_string).split(\"-\")\n",
    "            year = str(int(date_string_splitted[0]))\n",
    "            month = str(int(date_string_splitted[1]))\n",
    "            day = str(int(date_string_splitted[2]))\n",
    "            hour = str(int(fields[-1].split(\"-\")[0]))\n",
    "            flow = int(fields[0])\n",
    "            if (origin_hcd, destination_hcd, hour, day, month, year) not in od_book:\n",
    "                od_book[(origin_hcd, destination_hcd, hour, day, month, year)] = 0\n",
    "            od_book[(origin_hcd, destination_hcd, hour, day, month, year)] += flow\n",
    "    return(od_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def od_book_to_od_dataframe(telia_od_book):\n",
    "    #trips_sum,date,origin_muni_code,origin_muni,origin_region_code,origin_region,dest_muni_code,dest_muni,dest_region_code,dest_region,hour_bin\n",
    "    list_of_lists = [] \n",
    "    import pandas as pd  \n",
    "    \n",
    "    #telia_hcd_od[\"Ã…land\", \"Helsinki and Uusimaa Hospital District\", \"6\", \"10\", \"2\", \"2019\"]\n",
    "    header = ['origin', 'destination', 'hour', 'day', 'month', 'year', 'day_of_the_week', 'time_string', 'flow']\n",
    "    for key in telia_od_book.keys():\n",
    "        origin, destination, hour, day, month, year = key[0], key[1], key[2], key[3], key[4], key[5]\n",
    "        weekday = date_to_weekday(int(day), int(month), int(year))\n",
    "        flow = telia_od_book[key]\n",
    "        time_string = hour + \" \" + (\"-\").join([year, month, day])\n",
    "        list_of_lists.append([origin, destination, hour, day, month, year, weekday, time_string, flow])\n",
    "    df = pd.DataFrame(list_of_lists, columns = header)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "telia_hcd_od_book = telia_od_book_hcd_level(telia_datapath, m_to_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "telia_hcd_od_book_2019 = {key:value for (key, value) in telia_hcd_od_book.items() if key[5] == \"2019\"}\n",
    "telia_hcd_od_book_2020 = {key:value for (key, value) in telia_hcd_od_book.items() if key[5] == \"2020\"}\n",
    "#pickle the ground truth\n",
    "pickle_path = \"/.../telia_ground_truth.pkl\"\n",
    "with open(pickle_path, 'wb') as handle:\n",
    "    pickle.dump(telia_hcd_od_book_2020, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telia_hcd_od_dataframe = od_book_to_od_dataframe(telia_hcd_od_book)\n",
    "telia_hcd_od_dataframe_2019 = telia_hcd_od_dataframe.loc[telia_hcd_od_dataframe['year'] == \"2019\"]\n",
    "telia_hcd_od_dataframe_2020 = telia_hcd_od_dataframe.loc[telia_hcd_od_dataframe['year'] == \"2020\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breaking_outgoing_telia_flow_to_fractions_and_make_dataframe(telia_od_df):\n",
    "    header = ['origin', 'destination', 'hour', 'day', 'month', 'year', 'day_of_the_week', 'time_string', 'flow', 'fraction' ]\n",
    "    unique_timestring_list = telia_od_df.time_string.unique().tolist()\n",
    "    fractions_list_of_lists = []\n",
    "    telia_od_diag_removed = telia_od_df.loc[telia_od_df['destination'] != telia_od_df['origin']]\n",
    "    unique_origins = telia_od_diag_removed.origin.unique().tolist()\n",
    "    for o in unique_origins:\n",
    "        print(o)\n",
    "        selected_origin = telia_od_diag_removed.loc[telia_od_diag_removed['origin'] == o]\n",
    "        for t in unique_timestring_list:\n",
    "            selected_origin_time = selected_origin.loc[selected_origin['time_string'] == t]\n",
    "            total_flow = selected_origin_time['flow'].sum()\n",
    "            slected_list_of_lists = selected_origin_time.values.tolist()\n",
    "            for ls in slected_list_of_lists:\n",
    "                fraction = ls[-1]/total_flow\n",
    "                ls.append(fraction)\n",
    "                fractions_list_of_lists.append(ls)\n",
    "    fractions_df = pd.DataFrame(fractions_list_of_lists, columns = header)\n",
    "    return(fractions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_frac_df_2019 = breaking_outgoing_telia_flow_to_fractions_and_make_dataframe(telia_hcd_od_dataframe_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breaking_incomming_telia_flow_to_fractions_and_make_dataframe(telia_od_df):\n",
    "    header = ['origin', 'destination', 'hour', 'day', 'month', 'year', 'day_of_the_week', 'time_string', 'flow', 'fraction']\n",
    "    telia_od_diag_removed = telia_od_df.loc[telia_od_df['destination'] != telia_od_df['origin']]\n",
    "    unique_timestring_list = telia_od_diag_removed.time_string.unique().tolist()\n",
    "    print(len(unique_timestring_list))\n",
    "    unique_destinations = telia_od_diag_removed.destination.unique().tolist()\n",
    "    print(len(unique_destinations))\n",
    "    fractions_list_of_lists = []\n",
    "    for d in unique_destinations:\n",
    "        print(d)\n",
    "        selected_destination = telia_od_diag_removed.loc[telia_od_diag_removed['destination'] == d]\n",
    "        for t in unique_timestring_list:\n",
    "            selected_destination_time = selected_destination.loc[selected_destination['time_string'] == t]\n",
    "            total_flow = selected_destination_time['flow'].sum()\n",
    "            slected_list_of_lists = selected_destination_time.values.tolist()\n",
    "            for ls in slected_list_of_lists:\n",
    "                fraction = ls[-1]/total_flow\n",
    "                ls.append(fraction)\n",
    "                fractions_list_of_lists.append(ls)\n",
    "    fractions_df = pd.DataFrame(fractions_list_of_lists, columns = header)\n",
    "    return(fractions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_frac_df_2019 = breaking_incomming_telia_flow_to_fractions_and_make_dataframe(telia_hcd_od_dataframe_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_regional_in_frac_df(in_frac_df):\n",
    "    # this function is called inside those functions which calculate the median of fractions\n",
    "    book_of_dataframes = {}\n",
    "    unique_destinations = in_frac_df.destination.unique().tolist()\n",
    "    for d in unique_destinations:\n",
    "        selected_destination = in_frac_df.loc[in_frac_df['destination'] == d]\n",
    "        book_of_dataframes[d] = selected_destination\n",
    "    return(book_of_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_regional_out_frac_df(out_frac_df):\n",
    "    # this function is called inside those functions which calculate the median of fractions\n",
    "    book_of_dataframes = {}\n",
    "    unique_origins = out_frac_df.origin.unique().tolist()\n",
    "    for o in unique_origins:\n",
    "        selected_origin = out_frac_df.loc[out_frac_df['origin'] == o]\n",
    "        book_of_dataframes[o] = selected_origin\n",
    "    return(book_of_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weekday_frac_df(df):\n",
    "    book_of_dataframes = {}\n",
    "    unique_weekdays = df.day_of_the_week.unique().tolist()\n",
    "    for w in unique_weekdays:\n",
    "        selected_weekday = df.loc[df['day_of_the_week'] == w]\n",
    "        book_of_dataframes[w] = selected_weekday\n",
    "    return(book_of_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_month_frac_df(df):\n",
    "    book_of_dataframes = {}\n",
    "    unique_months = df.month.unique().tolist()\n",
    "    for m in unique_months:\n",
    "        selected_month = df.loc[df['month'] == m]\n",
    "        book_of_dataframes[m] = selected_month\n",
    "    return(book_of_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hour_frac_df(df):\n",
    "    book_of_dataframes = {}\n",
    "    unique_hours = df.hour.unique().tolist()\n",
    "    for h in unique_hours:\n",
    "        #print(h)\n",
    "        selected_hour = df.loc[df['hour'] == h]\n",
    "        book_of_dataframes[h] = selected_hour\n",
    "    return(book_of_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_different_years(fraction_df):\n",
    "    book_of_dataframes = {}\n",
    "    year_list = ['2019', '2020']\n",
    "    for y in year_list:\n",
    "        selected_lines = fraction_df.loc[fraction_df['year'] == y]\n",
    "        book_of_dataframes[y] = selected_lines\n",
    "    return(book_of_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_out_fraction_hour_and_weekday(out_fraction_df):\n",
    "    print(\"dataframe length:\")\n",
    "    print(len(out_fraction_df))\n",
    "    median_book = {}\n",
    "    separated_by_o = make_regional_out_frac_df(out_fraction_df)\n",
    "    for o in separated_by_o.keys():\n",
    "        separated_by_o_w = make_weekday_frac_df(separated_by_o[o])\n",
    "        for w in separated_by_o_w.keys():\n",
    "            separated_by_o_w_h = make_hour_frac_df(separated_by_o_w[w])\n",
    "            for h in separated_by_o_w_h.keys():\n",
    "                small_df = separated_by_o_w_h[h]\n",
    "                unique_destinations = small_df.destination.unique().tolist()\n",
    "                unnormalized_mobility_vector = dict()\n",
    "                for d in unique_destinations:\n",
    "                    filtered_by_destination = small_df.loc[small_df['destination'] == d]\n",
    "                    data_points = len(filtered_by_destination)\n",
    "                    median = filtered_by_destination['fraction'].median()\n",
    "                    unnormalized_mobility_vector[d] = median\n",
    "                    #renormalize_the_signature\n",
    "                sum_unnormalized_mobility_vector = sum(list(unnormalized_mobility_vector.values()))\n",
    "                for key in unnormalized_mobility_vector:\n",
    "                    median_book[(o, key, h, w)] = unnormalized_mobility_vector[key]/sum_unnormalized_mobility_vector                   \n",
    "    return(median_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_in_fraction_hour_and_weekday(in_fraction_df):\n",
    "    median_book = {}\n",
    "    print(\"dataframe length:\")\n",
    "    print(len(in_fraction_df))\n",
    "    counter = 0\n",
    "    separated_by_d = make_regional_in_frac_df(in_fraction_df)\n",
    "    for d in separated_by_d.keys():\n",
    "        separated_by_d_w = make_weekday_frac_df(separated_by_d[d])\n",
    "        for w in separated_by_d_w.keys():\n",
    "            separated_by_d_w_h = make_hour_frac_df(separated_by_d_w[w])\n",
    "            for h in separated_by_d_w_h.keys():\n",
    "                small_df = separated_by_d_w_h[h]\n",
    "                unique_origins = small_df.origin.unique().tolist()\n",
    "                unnormalized_mobility_vector = dict()\n",
    "                for o in unique_origins:\n",
    "                    filtered_by_origin = small_df.loc[small_df['origin'] == o]\n",
    "                    data_points = len(filtered_by_origin)\n",
    "                    median = filtered_by_origin['fraction'].median()\n",
    "                    unnormalized_mobility_vector[o] = median\n",
    "                    #renormalize_the_signature\n",
    "                sum_unnormalized_mobility_vector = sum(list(unnormalized_mobility_vector.values()))\n",
    "                for key in unnormalized_mobility_vector:\n",
    "                    median_book[(key, d, h, w)] = unnormalized_mobility_vector[key]/sum_unnormalized_mobility_vector\n",
    "                    counter += 1\n",
    "    return(median_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_out_fraction_basic(out_fraction_df):\n",
    "    w_list = out_fraction_df.day_of_the_week.unique().tolist()\n",
    "    h_list = out_fraction_df.hour.unique().tolist()\n",
    "    print(\"dataframe length:\")\n",
    "    print(len(out_fraction_df))\n",
    "    median_book = {}\n",
    "    separated_by_o = make_regional_out_frac_df(out_fraction_df)\n",
    "    for o in separated_by_o.keys():\n",
    "        small_df = separated_by_o[o]\n",
    "        unique_destinations = small_df.destination.unique().tolist()\n",
    "        unnormalized_mobility_vector = dict()\n",
    "        for d in unique_destinations:\n",
    "            filtered_by_destination = small_df.loc[small_df['destination'] == d]\n",
    "            median = filtered_by_destination['fraction'].median()\n",
    "            unnormalized_mobility_vector[d] = median\n",
    "        #renormalize_the_signature\n",
    "        sum_unnormalized_mobility_vector = sum(list(unnormalized_mobility_vector.values()))\n",
    "        for key in unnormalized_mobility_vector:\n",
    "            for w in w_list:\n",
    "                for h in h_list:\n",
    "                    median_book[(o, key, h, w)] = unnormalized_mobility_vector[key]/sum_unnormalized_mobility_vector\n",
    "            \n",
    "    return(median_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_in_fraction_basic(in_fraction_df):\n",
    "    w_list = in_fraction_df.day_of_the_week.unique().tolist()\n",
    "    h_list = in_fraction_df.hour.unique().tolist()\n",
    "    median_book = {}\n",
    "    print(\"dataframe length:\")\n",
    "    print(len(in_fraction_df))\n",
    "    separated_by_d = make_regional_in_frac_df(in_fraction_df)\n",
    "    for d in separated_by_d.keys():\n",
    "        small_df = separated_by_d[d]\n",
    "        unique_origins = small_df.origin.unique().tolist()\n",
    "        unnormalized_mobility_vector = dict()\n",
    "        for o in unique_origins:\n",
    "            filtered_by_origin = small_df.loc[small_df['origin'] == o]\n",
    "            median = filtered_by_origin['fraction'].median()\n",
    "            unnormalized_mobility_vector[o] = median\n",
    "        #renormalize_the_signature\n",
    "        sum_unnormalized_mobility_vector = sum(list(unnormalized_mobility_vector.values()))\n",
    "        for key in unnormalized_mobility_vector:\n",
    "            for w in w_list:\n",
    "                for h in h_list:\n",
    "                    median_book[(key, d, h, w)] = unnormalized_mobility_vector[key]/sum_unnormalized_mobility_vector\n",
    "    return(median_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_in_fraction_hour(in_fraction_df):\n",
    "    median_book = {}\n",
    "    w_list = in_fraction_df.day_of_the_week.unique().tolist()\n",
    "    print(\"dataframe length:\")\n",
    "    print(len(in_fraction_df))\n",
    "    separated_by_d = make_regional_in_frac_df(in_fraction_df)\n",
    "    for d in separated_by_d.keys():\n",
    "        separated_by_d_h = make_hour_frac_df(separated_by_d[d])\n",
    "        for h in separated_by_d_h.keys():\n",
    "            small_df = separated_by_d_h[h]\n",
    "            unique_origins = small_df.origin.unique().tolist()\n",
    "            unnormalized_mobility_vector = dict()\n",
    "            for o in unique_origins:\n",
    "                filtered_by_origin = small_df.loc[small_df['origin'] == o]\n",
    "                median = filtered_by_origin['fraction'].median()\n",
    "                unnormalized_mobility_vector[o] = median\n",
    "            #renormalize_the_signature\n",
    "            sum_unnormalized_mobility_vector = sum(list(unnormalized_mobility_vector.values()))\n",
    "            for key in unnormalized_mobility_vector:\n",
    "                for w in w_list:\n",
    "                    median_book[(key, d, h, w)] = unnormalized_mobility_vector[key]/sum_unnormalized_mobility_vector\n",
    "    return(median_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_out_fraction_hour(out_fraction_df):\n",
    "    w_list = out_fraction_df.day_of_the_week.unique().tolist()\n",
    "    print(\"dataframe length:\")\n",
    "    print(len(out_fraction_df))\n",
    "    median_book = {}\n",
    "    separated_by_o = make_regional_out_frac_df(out_fraction_df)\n",
    "    for o in separated_by_o.keys():\n",
    "        separated_by_o_h = make_hour_frac_df(separated_by_o[o])\n",
    "        for h in separated_by_o_h.keys():\n",
    "            small_df = separated_by_o_h[h]\n",
    "            unique_destinations = small_df.destination.unique().tolist()\n",
    "            unnormalized_mobility_vector = dict()\n",
    "            for d in unique_destinations:\n",
    "                filtered_by_destination = small_df.loc[small_df['destination'] == d]\n",
    "                median = filtered_by_destination['fraction'].median()\n",
    "                unnormalized_mobility_vector[d] = median\n",
    "            #renormalize_the_signature\n",
    "            sum_unnormalized_mobility_vector = sum(list(unnormalized_mobility_vector.values()))\n",
    "            for key in unnormalized_mobility_vector:\n",
    "                for w in w_list:\n",
    "                    median_book[(o, key, h, w)] = unnormalized_mobility_vector[key]/sum_unnormalized_mobility_vector\n",
    "    return(median_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_in_fraction_weekday(in_fraction_df):\n",
    "    h_list = in_fraction_df.hour.unique().tolist()\n",
    "    median_book = {}\n",
    "    print(\"dataframe length:\")\n",
    "    print(len(in_fraction_df))\n",
    "    counter = 0\n",
    "    separated_by_d = make_regional_in_frac_df(in_fraction_df)\n",
    "    for d in separated_by_d.keys():\n",
    "        separated_by_d_w = make_weekday_frac_df(separated_by_d[d])\n",
    "        for w in separated_by_d_w.keys():\n",
    "            small_df = separated_by_d_w[w]\n",
    "            unique_origins = small_df.origin.unique().tolist()\n",
    "            unnormalized_mobility_vector = dict()\n",
    "            for o in unique_origins:\n",
    "                filtered_by_origin = small_df.loc[small_df['origin'] == o]\n",
    "                median = filtered_by_origin['fraction'].median()\n",
    "                unnormalized_mobility_vector[o] = median\n",
    "            #renormalize_the_signature\n",
    "            sum_unnormalized_mobility_vector = sum(list(unnormalized_mobility_vector.values()))\n",
    "            for key in unnormalized_mobility_vector:\n",
    "                for h in h_list:\n",
    "                    median_book[(key, d, h, w)] = unnormalized_mobility_vector[key]/sum_unnormalized_mobility_vector\n",
    "    return(median_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_out_fraction_weekday(out_fraction_df):\n",
    "    h_list = out_fraction_df.hour.unique().tolist()\n",
    "    print(\"dataframe length:\")\n",
    "    print(len(out_fraction_df))\n",
    "    median_book = {}\n",
    "    counter = 0\n",
    "    encountered_keys = 0\n",
    "    separated_by_o = make_regional_out_frac_df(out_fraction_df)\n",
    "    for o in separated_by_o.keys():\n",
    "        separated_by_o_w = make_weekday_frac_df(separated_by_o[o])\n",
    "        for w in separated_by_o_w.keys():\n",
    "            small_df = separated_by_o_w[w]\n",
    "            unique_destinations = small_df.destination.unique().tolist()\n",
    "            unnormalized_mobility_vector = dict()\n",
    "            for d in unique_destinations:\n",
    "                #filtered_by_destination = small_df.loc[small_df['destination'] != d]\n",
    "                filtered_by_destination = small_df.loc[small_df['destination'] == d]\n",
    "                median = filtered_by_destination['fraction'].median()\n",
    "                unnormalized_mobility_vector[d] = median\n",
    "            #renormalize_the_signature\n",
    "            sum_unnormalized_mobility_vector = sum(list(unnormalized_mobility_vector.values()))\n",
    "            for key in unnormalized_mobility_vector:\n",
    "                for h in h_list:\n",
    "                    median_book[(o, key, h, w)] = unnormalized_mobility_vector[key]/sum_unnormalized_mobility_vector\n",
    "    return(median_book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_hcd_indegree_path = os.path.join(dir_path, 'road_traffic_data') + \"/road_hcd_outdegree.pkl\"\n",
    "road_hcd_outdegree_path = os.path.join(dir_path, 'road_traffic_data') + \"/road_hcd_outdegree.pkl\"\n",
    "with open(road_hcd_indegree_path,'rb') as f:\n",
    "     road_indegree_book = pickle.load(f)\n",
    "with open(road_hcd_outdegree_path,'rb') as f:\n",
    "     road_outdegree_book = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_indegree_book_2020 = {key:value for (key, value) in road_indegree_book.items() if key[4] == \"2020\"}\n",
    "road_indegree_book_2019 = {key:value for (key, value) in road_indegree_book.items() if key[4] == \"2019\"}\n",
    "road_outdegree_book_2020 = {key:value for (key, value) in road_outdegree_book.items() if key[4] == \"2020\"}\n",
    "road_outdegree_book_2019 = {key:value for (key, value) in road_outdegree_book.items() if key[4] == \"2019\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Telia and Road Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'telia_hcd_od_book' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7934/2284755915.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mroad_hcd_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroad_indegree_book_2020\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtelia_hcd_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtelia_hcd_od_book\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcommon_hcd_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroad_hcd_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtelia_hcd_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon_hcd_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'telia_hcd_od_book' is not defined"
     ]
    }
   ],
   "source": [
    "road_hcd_set = set([key[0] for key in list(road_indegree_book_2020.keys())])\n",
    "telia_hcd_set = set([key[0] for key in list(telia_hcd_od_book.keys())])   \n",
    "common_hcd_list = road_hcd_set.intersection(telia_hcd_set)\n",
    "print(common_hcd_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_degree_dictionary_and_make_a_dataframe_of_their_mutual_rows(road_degree_dict, telia_degree_dict):\n",
    "    list_of_lists = []\n",
    "    df_header = ['region', 'hour', 'day', 'month', 'year', \"day_of_the_week\", 'timestamp', 'road_degree', 'telia_degree']\n",
    "    mutual_keys_list = list(set(telia_degree_dict.keys()).intersection(set(road_degree_dict.keys())))\n",
    "    for key in mutual_keys_list:\n",
    "        (region, hour, day, month, year) = key\n",
    "        #timestring = hour + \" \" + (\"-\").join([year, month, day])\n",
    "        datetime_string = year+\"-\"+month+\"-\"+day+\" \"+hour+\":00\"       \n",
    "        weekday = date_to_weekday(int(day), int(month), int(year))\n",
    "        list_to_append = [region, hour, day, month, year, weekday, datetime_string, road_degree_dict[key], telia_degree_dict[key]]\n",
    "        list_of_lists.append(list_to_append)\n",
    "    df = pd.DataFrame(list_of_lists, columns = df_header)\n",
    "    return(df)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree_road_and_telia_df_2020 = get_two_degree_dictionary_and_make_a_dataframe_of_their_mutual_rows(road_indegree_book_2020, hcd_telia_indegree_book)\n",
    "in_degree_road_and_telia_df_2019 = get_two_degree_dictionary_and_make_a_dataframe_of_their_mutual_rows(road_indegree_book_2019, hcd_telia_indegree_book)\n",
    "out_degree_road_and_telia_df_2020 = get_two_degree_dictionary_and_make_a_dataframe_of_their_mutual_rows(road_outdegree_book_2020, hcd_telia_outdegree_book)\n",
    "out_degree_road_and_telia_df_2019 = get_two_degree_dictionary_and_make_a_dataframe_of_their_mutual_rows(road_outdegree_book_2019, hcd_telia_outdegree_book)\n",
    "in_degree_road_and_telia_df_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_regional_degree_df(degree_df):\n",
    "    book_of_dataframes = {}\n",
    "    unique_regions = degree_df.region.unique().tolist()\n",
    "    for r in unique_regions:\n",
    "        selected_lines = degree_df.loc[degree_df['region'] == r]\n",
    "        book_of_dataframes[r] = selected_lines\n",
    "    return(book_of_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_different_hours(road_and_telia_degree_dataframe):\n",
    "    book_of_dataframes = {}\n",
    "    hour_list = ['0', '6', '12', '18']\n",
    "    for h in hour_list:\n",
    "        selected_lines = road_and_telia_degree_dataframe.loc[road_and_telia_degree_dataframe['hour'] == h]\n",
    "        book_of_dataframes[h] = selected_lines\n",
    "    return(book_of_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_different_weekdays(road_and_telia_degree_dataframe):\n",
    "    df = road_and_telia_degree_dataframe\n",
    "    book_of_dataframes = {}\n",
    "    weekday_list = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "    for weekday in weekday_list:\n",
    "        mask = (df['day_of_the_week'] == weekday)\n",
    "        related_lines = road_and_telia_degree_dataframe.loc[mask]\n",
    "        book_of_dataframes[weekday] = related_lines\n",
    "    return(book_of_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_different_hours_and_different_weekdays(road_and_telia_degree_dataframe):\n",
    "    book_of_dataframes = {}\n",
    "    hour_list = ['0', '6', '12', '18']\n",
    "    weekday_list = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "    for h in hour_list:\n",
    "        for wd in weekday_list:\n",
    "            h_lines = road_and_telia_degree_dataframe.loc[road_and_telia_degree_dataframe['hour'] == h]\n",
    "            selected_lines = h_lines.loc[h_lines['day_of_the_week'] == wd]\n",
    "            book_of_dataframes[(h, wd)] = selected_lines\n",
    "    return(book_of_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekend_and_weekdays(road_and_telia_degree_dataframe):\n",
    "    df = road_and_telia_degree_dataframe\n",
    "    book_of_dataframes = {}\n",
    "    weekend_mask = (df['day_of_the_week'] == \"Saturday\") | (df['day_of_the_week'] == \"Sunday\")\n",
    "    weekday_mask = (df['day_of_the_week'] != \"Saturday\") & (df['day_of_the_week'] != \"Sunday\")\n",
    "    #weekend_lines = road_and_telia_degree_dataframe.loc[road_and_telia_degree_dataframe['day_of_the_week'] in {\"Saturday\", \"Sunday\"}]\n",
    "    weekend_lines = road_and_telia_degree_dataframe.loc[weekend_mask]\n",
    "    weekday_lines = road_and_telia_degree_dataframe.loc[weekday_mask]\n",
    "    book_of_dataframes[\"weekend\"] = weekend_lines\n",
    "    book_of_dataframes[\"weekday\"] = weekday_lines\n",
    "    return(book_of_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_transform_from_road_degree_to_telia_degree(filter_function, road_and_telia_degree_dataframe):\n",
    "    \n",
    "    transformation_values_book = {}\n",
    "    if filter_function == None:\n",
    "        dict_of_flitered_dataframes = {\"all\": road_and_telia_degree_dataframe}\n",
    "    else:\n",
    "        dict_of_flitered_dataframes = filter_function(road_and_telia_degree_dataframe)\n",
    "    for label in dict_of_flitered_dataframes.keys():\n",
    "        flitered_df = dict_of_flitered_dataframes[label]\n",
    "        book_of_dataframes_indexed_by_region = make_regional_degree_df(flitered_df)\n",
    "        for hcd in book_of_dataframes_indexed_by_region.keys():\n",
    "            hcd_filterd_df = book_of_dataframes_indexed_by_region[hcd]\n",
    "            road_degree_list = hcd_filterd_df['road_degree'].tolist()\n",
    "            telia_degree_list = hcd_filterd_df['telia_degree'].tolist()\n",
    "            region_list = hcd_filterd_df['region'].tolist()\n",
    "            hour_list = hcd_filterd_df['hour'].tolist()\n",
    "            day_list = hcd_filterd_df['day'].tolist()\n",
    "            month_list = hcd_filterd_df['month'].tolist()\n",
    "            year_list = hcd_filterd_df['year'].tolist()\n",
    "            correlation_coefficient, p_value = stat.pearsonr(telia_degree_list, road_degree_list)\n",
    "            m, b = np.polyfit(road_degree_list, telia_degree_list, 1)\n",
    "            for i in range(len(region_list)):\n",
    "                reg = region_list[i]\n",
    "                hour = hour_list[i]\n",
    "                day = day_list[i]\n",
    "                month = month_list[i]\n",
    "                year = year_list[i]\n",
    "                weekday = date_to_weekday(day, month, year) ## this is s atupidly expensive way of doing this, correct it later\n",
    "                transformation_values_book[(reg, hour, weekday)] = (correlation_coefficient, p_value, m, b)\n",
    "    return(transformation_values_book)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linear_transformation_book_indegree_2019 = linear_transform_from_road_degree_to_telia_degree(divide_different_hours, in_degree_road_and_telia_df_2019)\n",
    "linear_transformation_book_outdegree_2019 = linear_transform_from_road_degree_to_telia_degree(divide_different_hours, out_degree_road_and_telia_df_2019)\n",
    "linear_transformation_book_indegree_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree_road_and_telia_df_2020['parsed_time'] = pd.to_datetime(in_degree_road_and_telia_df_2020['timestamp'])\n",
    "#in_degree_road_and_telia_df_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree_road_and_telia_df_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_flow(keys_to_predict_for, in_road_degree_book, indegree_transformation_book, median_in_frac_book, out_road_degree_book, outdegree_transformation_book, median_out_frac_book, path_to_save_results = None):\n",
    "    outdegree_trans_key_set = set(outdegree_transformation_book.keys())\n",
    "    indegree_trans_key_set = set(indegree_transformation_book.keys())\n",
    "    out_road_degree_keys_set = set(out_road_degree_book.keys())\n",
    "    in_road_degree_keys_set = set(in_road_degree_book.keys())\n",
    "    key_list_to_loop_over = []\n",
    "    for (origin, destination, hour, day, month, year) in keys_to_predict_for:\n",
    "        #print(len(keys_to_predict_for))\n",
    "        if (origin, hour, day, month, year) in out_road_degree_keys_set:\n",
    "            #print(len(out_road_degree_keys_set))\n",
    "            if (destination, hour, day, month, year) in in_road_degree_keys_set:\n",
    "                #print(len(in_road_degree_keys_set))\n",
    "                weekday = date_to_weekday(day, month, year) #figure out the weekday\n",
    "                if (origin, hour, weekday) in outdegree_trans_key_set:\n",
    "                    #print(len(outdegree_trans_key_set))\n",
    "                    if (destination, hour, weekday) in indegree_trans_key_set:\n",
    "                        if (origin, destination, hour, weekday) in median_out_frac_book:\n",
    "                            if (origin, destination, hour, weekday) in median_in_frac_book:\n",
    "                                key_list_to_loop_over.append((origin, destination, hour, day, month, year))                        \n",
    "    \n",
    "    \n",
    "    \n",
    "    out_pred_book = {}\n",
    "    in_pred_book = {}\n",
    "    comb_pred_book = {}\n",
    "    for (origin, destination, hour, day, month, year) in key_list_to_loop_over:\n",
    "        weekday = date_to_weekday(day, month, year) #figure out the weekday\n",
    "        out_key = (origin, hour, day, month, year) \n",
    "        #trans_tuple = outdegree_transformation_book[out_key]\n",
    "        trans_tuple = outdegree_transformation_book[(origin, hour, weekday)]\n",
    "        m_out, b_out = trans_tuple[2], trans_tuple[3]\n",
    "        f_out = median_out_frac_book[(origin, destination, hour, weekday)]\n",
    "        out_pred = (out_road_degree_book[out_key] * m_out + b_out) * f_out\n",
    "        out_pred_book[(origin, destination, hour, day, month, year)] = out_pred\n",
    "        \n",
    "        in_key = (destination, hour, day, month, year)\n",
    "        #trans_tuple = indegree_transformation_book[in_key]\n",
    "        trans_tuple = indegree_transformation_book[(destination, hour, weekday)]\n",
    "        m_in, b_in = trans_tuple[2], trans_tuple[3]\n",
    "        f_in = median_in_frac_book[(origin, destination, hour, weekday)]\n",
    "        in_pred = (in_road_degree_book[in_key] * m_in + b_in) * f_in\n",
    "        in_pred_book[(origin, destination, hour, day, month, year)] = in_pred\n",
    "        #from IPython.core.debugger import Pdb\n",
    "        #Pdb().set_trace()\n",
    "    for key in in_pred_book.keys():\n",
    "        if key in out_pred_book.keys():\n",
    "            comb_pred = (in_pred_book[key] + out_pred_book[key])/2\n",
    "            comb_pred_book[key] = comb_pred\n",
    "            \n",
    "    if path_to_save_results:\n",
    "        print(\"printing\")\n",
    "        with open(path_to_save_results, 'w') as results_file:\n",
    "            header_list = [\"flow_estimate\", \"timestamp\", \"date\", \"day_of_the_week\", \"hour\", \"origin_hcd\", \"destination_hcd\"]\n",
    "            header_to_write = \",\".join(header_list)+\"\\n\"\n",
    "            results_file.write(header_to_write)\n",
    "            for key in comb_pred_book.keys():\n",
    "                hour = str(key[2])\n",
    "                day = str(key[3])\n",
    "                month = str(key[4])\n",
    "                year = str(key[5])\n",
    "                #date_string = str(key[5])+\"-\"+str(key[4])+\"-\"+str(key[3])\n",
    "                date_string = year+\"-\"+month+\"-\"+day\n",
    "                weekday = date_to_weekday(day, month, year)\n",
    "                datetime_string = year+\"-\"+month+\"-\"+day+\" \"+hour+\":00\"\n",
    "                list_to_write = [str(comb_pred_book[key]), datetime_string, date_string, weekday, hour, key[0], key[1]]\n",
    "                line_to_write = \",\".join(list_to_write)+\"\\n\"\n",
    "                results_file.write(line_to_write)     \n",
    "    \n",
    "    return(in_pred_book, out_pred_book, comb_pred_book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Telia Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_flow_2019(telia_od_df_2019):\n",
    "    print(\"dataframe length:\")\n",
    "    print(len(telia_od_df_2019))\n",
    "    median_book = {}\n",
    "    df = telia_od_df_2019\n",
    "    counter = 0\n",
    "    encountered_keys = 0\n",
    "    separated_by_d = make_regional_in_frac_df(df)\n",
    "    no_datapoint_falg = False\n",
    "    for d in separated_by_d.keys():\n",
    "        separated_by_d_w = make_weekday_frac_df(separated_by_d[d])\n",
    "        for w in separated_by_d_w.keys():\n",
    "            separated_by_d_w_h = make_hour_frac_df(separated_by_d_w[w])\n",
    "            for h in separated_by_d_w_h.keys():\n",
    "                small_df = separated_by_d_w_h[h]\n",
    "                unique_origins = small_df.origin.unique().tolist()\n",
    "                for o in unique_origins:\n",
    "                    filtered_by_origin = small_df.loc[small_df['origin'] == o]\n",
    "                    selected_lines = filtered_by_origin\n",
    "                    data_points = len(selected_lines)\n",
    "                    if data_points == 0: #if there are no other datapaoints with the same origin, destination, hour, and weekday, return the median withouth filtering with regards to weekday\n",
    "                        no_datapoint_flag = True\n",
    "                    elif data_points == 1:\n",
    "                        median = selected_lines['flow'].values[0]\n",
    "                    else:\n",
    "                        median = selected_lines['flow'].median()\n",
    "                    if no_datapoint_falg != True:\n",
    "                        if (o, d, h, w) in median_book:\n",
    "                            encountered_keys += 1\n",
    "                        median_book[(o, d, h, w)] = median \n",
    "                        counter += 1\n",
    "                    no_datapoint_flag = False\n",
    "    print(\"how many times the innest part of the loop is reached\")\n",
    "    print(counter)\n",
    "    print(\"repeated keys\")\n",
    "    print(encountered_keys)\n",
    "    return(median_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_prediction(telia_hcd_od_book, median_od_book_2019, path_to_save_results = None):\n",
    "    pred_book = {}\n",
    "    not_found = 0\n",
    "    for (origin, destination, hour, day, month, year) in telia_hcd_od_book.keys():\n",
    "        if year == \"2020\":\n",
    "            weekday = date_to_weekday(day, month, year)\n",
    "            if (origin, destination, hour, weekday) in median_od_book_2019.keys():\n",
    "                pred = median_od_book_2019[(origin, destination, hour, weekday)]\n",
    "                pred_book[(origin, destination, hour, day, month, year)] = pred\n",
    "            else:\n",
    "                not_found += 1\n",
    "    if path_to_save_results:\n",
    "        print(\"printing\")\n",
    "        with open(path_to_save_results, 'w') as results_file:\n",
    "            header_list = [\"flow_estimate\", \"timestamp\", \"date\", \"day_of_the_week\", \"hour\", \"origin_hcd\", \"destination_hcd\"]\n",
    "            header_to_write = \",\".join(header_list)+\"\\n\"\n",
    "            results_file.write(header_to_write)\n",
    "            for key in pred_book.keys():\n",
    "                hour = str(key[2])\n",
    "                day = str(key[3])\n",
    "                month = str(key[4])\n",
    "                year = str(key[5])\n",
    "                date_string = year+\"-\"+month+\"-\"+day\n",
    "                weekday = date_to_weekday(day, month, year)\n",
    "                datetime_string = year+\"-\"+month+\"-\"+day+\" \"+hour+\":00\"\n",
    "                list_to_write = [str(pred_book[key]), datetime_string, date_string, weekday, hour, key[0], key[1]]\n",
    "                line_to_write = \",\".join(list_to_write)+\"\\n\"\n",
    "                results_file.write(line_to_write)   \n",
    "    return(pred_book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make baseline predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telia_hcd_od_dataframe_2019 = telia_hcd_od_dataframe.loc[telia_hcd_od_dataframe['year'] == \"2019\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_od_book_2019 = median_flow_2019(telia_hcd_od_dataframe_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_estimation = '/.../median_of_2019_as_a_baseline.csv'\n",
    "baseline_pediction_book_2020 = baseline_prediction(telia_hcd_od_book_2020, median_od_book_2019, path_to_save_estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different versions of combining road data and telia data in 2019 to predict od in 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree_road_and_telia_df = get_two_degree_dictionary_and_make_a_dataframe_of_their_mutual_rows(road_indegree_book_2019 , hcd_telia_indegree_book_2019)\n",
    "out_degree_road_and_telia_df = get_two_degree_dictionary_and_make_a_dataframe_of_their_mutual_rows(road_outdegree_book_2019 , hcd_telia_outdegree_book_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic:\n",
    "* Use 2019 data for training\n",
    "* linear regression: Only one line fitted\n",
    "* median of fractions: Medianed in the most basic way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the medians\n",
    "median_in_fraction_book_x_0 = median_in_fraction_basic(in_frac_df_2019)\n",
    "median_out_fraction_book_x_0 = median_out_fraction_basic(out_frac_df_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression\n",
    "linear_transform_book_indegree_0_x = linear_transform_from_road_degree_to_telia_degree(None, in_degree_road_and_telia_df_2019)\n",
    "linear_transform_book_outdegree_0_x = linear_transform_from_road_degree_to_telia_degree(None, out_degree_road_and_telia_df_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the medians\n",
    "median_in_book = median_in_fraction_book_x_0\n",
    "median_out_book = median_out_fraction_book_x_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_transform_book_in = linear_transform_book_indegree_0_x\n",
    "linear_transform_book_out = linear_transform_book_outdegree_0_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_predict = telia_hcd_od_book_2020.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_estimation = '/.../telia_road_estimation_basic.csv'\n",
    "_, _, _ = predict_flow(keys_to_predict , road_indegree_book, linear_transform_book_in, median_in_book, road_outdegree_book, \\\n",
    "                       linear_transform_book_out, median_out_book, path_to_save_estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily rythms:\n",
    "* Use 2019 data years for prediction\n",
    "* linear regression: Only one line fitted for each quarter of the day\n",
    "* median of fractions: Medianed among the datapoints which are in the same hour of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression\n",
    "linear_transform_book_indegree_2_x = linear_transform_from_road_degree_to_telia_degree(divide_different_hours, in_degree_road_and_telia_df_2019)\n",
    "linear_transform_book_outdegree_2_x = linear_transform_from_road_degree_to_telia_degree(divide_different_hours, out_degree_road_and_telia_df_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_transform_book_in = linear_transform_book_indegree_2_x\n",
    "linear_transform_book_out = linear_transform_book_outdegree_2_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the medians\n",
    "median_in_fraction_book_x_1 = median_in_fraction_hour(in_frac_df_2019)\n",
    "median_out_fraction_book_x_1 = median_out_fraction_hour(out_frac_df_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the medians\n",
    "median_in_book = median_in_fraction_book_x_1\n",
    "median_out_book = median_out_fraction_book_x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_estimation = '/.../telia_road_estimation_daily_rhythms.csv'\n",
    "_, _, _ = predict_flow(keys_to_predict, road_indegree_book, linear_transform_book_in, median_in_book, road_outdegree_book, \\\n",
    "                       linear_transform_book_out, median_out_book, path_to_save_estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekly:\n",
    "* use 2019 data fro training\n",
    "* linear regression: one line fitted for each day of week\n",
    "* median of fractions: Medianed among the datapoints which are in the same day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the medians\n",
    "median_out_fraction_book_x_2 = median_out_fraction_weekday(out_frac_df_2019)\n",
    "median_in_fraction_book_x_2 = median_in_fraction_weekday(in_frac_df_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the medians\n",
    "median_in_book = median_in_fraction_book_x_2\n",
    "median_out_book = median_out_fraction_book_x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_transform_book_in = linear_transform_from_road_degree_to_telia_degree(divide_different_weekdays, in_degree_road_and_telia_df_2019)\n",
    "linear_transform_book_out = linear_transform_from_road_degree_to_telia_degree(divide_different_weekdays, out_degree_road_and_telia_df_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_estimation = '/.../telia_road_estimation_weekly_rhythms.csv'\n",
    "_, _, _ = predict_flow(keys_to_predict, road_indegree_book, linear_transform_book_in, median_in_book, road_outdegree_book, \\\n",
    "                       linear_transform_book_out, median_out_book, path_to_save_estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## daily and weekly:\n",
    "* Use 2019 data for training\n",
    "* linear regression: one line fitted for each (hour, weekday)\n",
    "* median of fractions: Medianed among the datapoints which are in the same day of the week and same hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the medians\n",
    "median_out_fraction_book_x_3 = median_out_fraction_hour_and_weekday(out_frac_df_2019)\n",
    "median_in_fraction_book_x_3 = median_in_fraction_hour_and_weekday(in_frac_df_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_transform_book_in = linear_transform_from_road_degree_to_telia_degree(divide_different_hours_and_different_weekdays, in_degree_road_and_telia_df_2019)\n",
    "linear_transform_book_out = linear_transform_from_road_degree_to_telia_degree(divide_different_hours_and_different_weekdays, out_degree_road_and_telia_df_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the medians\n",
    "median_in_book = median_in_fraction_book_x_3\n",
    "median_out_book = median_out_fraction_book_x_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_estimation = '/.../telia_road_estimation_daily_and_weekly_rhythms.csv'\n",
    "_, _, _ = predict_flow(keys_to_predict, road_indegree_book, linear_transform_book_in, median_in_book, road_outdegree_book, \\\n",
    "                       linear_transform_book_out, median_out_book, path_to_save_estimation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3/anaconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
