{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime as dt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the repository we are working in so later is easier to read/write files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/cs/networks/heydars1/dynamic-mobility/data\n"
     ]
    }
   ],
   "source": [
    "dir_path = os.path.abspath('')\n",
    "data_path = os.path.join(dir_path, 'data')\n",
    "print(data_path)\n",
    "path_for_saving_estimations = os.path.join(dir_path, 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_weekday(day, month, year):\n",
    "    parsed_date = dt.date(int(year), int(month), int(day))\n",
    "    weekday_map = {0:\"Monday\", 1:\"Tuesday\", 2:\"Wednesday\", 3:\"Thursday\", 4:\"Friday\", 5:\"Saturday\", 6:\"Sunday\"}\n",
    "    return((weekday_map[parsed_date.weekday()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_out_in_traffic(traffic_between_adjacent_data_path, out_degree_save_path = None, in_degree_save_path = None):\n",
    "    ##header of thte data and a example line:\n",
    "    #origin, destination, day, month, year, hour, vehicle-type:1, vehicle-type:2, vehicle-type:3, vehicle-type:4, vehicle-type:5, vehicle-type:6, vehicle-type:7\n",
    "    #Central-Finland-Hospital-District,Central-Ostrobothnia-Hospital-District,1,2,2020,0,12.0,0.0,0.0,0.0,11.0,0.0,0.0\n",
    "    hcd_in_degree = {} # a dictionary with (destination, hour, day, month, year) as key and total in-degree as value\n",
    "    hcd_out_degree = {}\n",
    "    with open(traffic_between_adjacent_data_path, 'r') as data:\n",
    "        for line in data:\n",
    "            fields = line.strip().split(\",\")\n",
    "            origin = fields[0]\n",
    "            destination = fields[1]\n",
    "            day, month, year, hour = int(fields[2]), int(fields[3]), int(fields[4]), int(fields[5])\n",
    "            vehicle_count_list = [float(fields[6]), float(fields[7]), float(fields[8]), float(fields[9]), float(fields[10]), float(fields[11]), float(fields[12])]\n",
    "            if (destination, hour, day, month, year) not in hcd_in_degree.keys():\n",
    "                hcd_in_degree[(destination, hour, day, month, year)] = [0 for i in range(len(vehicle_count_list))]\n",
    "            updated_in = [a + b for a, b in zip(hcd_in_degree[(destination, hour, day, month, year)], vehicle_count_list)]\n",
    "            hcd_in_degree[(destination, hour, day, month, year)] = updated_in\n",
    "            if (origin, hour, day, month, year) not in hcd_out_degree.keys():\n",
    "                hcd_out_degree[(origin, hour, day, month, year)] = [0 for i in range(len(vehicle_count_list))]\n",
    "            updated_out = [a + b for a, b in zip(hcd_out_degree[(origin, hour, day, month, year)], vehicle_count_list)]\n",
    "            hcd_out_degree[(origin, hour, day, month, year)] = updated_out\n",
    "            \n",
    "    ### print the in degree and out degree to a file\n",
    "    if out_degree_save_path is not None:\n",
    "        out_key_list = list(hcd_out_degree.keys())\n",
    "        sorted_out_key_list = sorted(out_key_list,key=lambda x: (x[0], x[4], x[3], x[2], x[1]))\n",
    "        with open(out_degree_save_path, \"w+\") as out_save_file:\n",
    "            header = \"in-or-out, hcd, hour, day, month, year, type-1-count, type-2-count, type-3-count, type-4-count, type-5-count, type-6-count, type-7-count\"\n",
    "            out_save_file.write(header+\"\\n\")\n",
    "            for key in sorted_out_key_list:\n",
    "                out_save_file.write(\"outdegree,\"+','.join([str(key[0]),str(key[1]),str(key[2]),str(key[3]),str(key[4])])+','+','.join([str(i) for i in hcd_out_degree[key]])+\"\\n\")\n",
    "                \n",
    "                \n",
    "    if in_degree_save_path is not None:\n",
    "        in_key_list = list(hcd_in_degree.keys())\n",
    "        sorted_in_key_list = sorted(in_key_list,key=lambda x: (x[0], x[3], x[2], x[1]))\n",
    "        output_file = open(in_degree_save_path, \"w+\")\n",
    "        header = \"in-or-out, hcd, hour, day, month, year, type-1-count, type-2-count, type-3-count, type-4-count, type-5-count, type-6-count, type-7-count\"\n",
    "        output_file.write(header+\"\\n\")\n",
    "        for key in sorted_in_key_list:\n",
    "            output_file.write(\"indegree,\"+','.join([str(key[0]),str(key[1]),str(key[2]),str(key[3]),str(key[4])])+','+ ','.join([str(i) for i in hcd_in_degree[key]])+\"\\n\")\n",
    "        \n",
    "    return(hcd_in_degree, hcd_out_degree)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we determine the path to road traffic data .csv file which is aggregated in hospital district care level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/cs/networks/heydars1/dynamic-mobility/road_traffic_data/hcd_2019_and_2020_4months_each_hcd_level_6h.csv\n"
     ]
    }
   ],
   "source": [
    "road_traffic_datapath = dir_path+ \"/road_traffic_data/hcd_2019_and_2020_4months_each_hcd_level_6h.csv\"\n",
    "print(road_traffic_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcd_traffic_in_degree, hcd_traffic_out_degree = calculate_total_out_in_traffic(road_traffic_datapath)\n",
    "\n",
    "# pickle the outdegree and indegree based on road traffic data. These pickles are used in some other scripts\n",
    "save_path = os.path.join(dir_path, 'road_traffic_data') + \"/road_hcd_indegree.pkl\"\n",
    "with open(save_path, 'wb') as handle:\n",
    "    pickle.dump(hcd_traffic_in_degree, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "save_path = os.path.join(dir_path, 'road_traffic_data') + \"/road_hcd_outdegree.pkl\"\n",
    "with open(save_path, 'wb') as handle:\n",
    "    pickle.dump(hcd_traffic_out_degree, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following function combines temporal road traffic data with the fractions from static radiation model to make a dynamic radiation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_road_traffic_with_static_fractions(in_road_traffic_count_dictionary, in_fractions_dictionary, out_road_traffic_count_dictionary, out_fractions_dictionary, passenger_per_car, path_to_save_results = None):\n",
    "    regions_in_road_traffic = set([tup[0] for tup in in_road_traffic_count_dictionary.keys()])\n",
    "    regions_in_static = set([tup[0] for tup in in_fractions_dictionary.keys()])\n",
    "    print(regions_in_road_traffic)\n",
    "    print(regions_in_static)\n",
    "    set_of_regions_in_both_data_sources = regions_in_road_traffic.intersection(regions_in_static)\n",
    "    region_list = list(set_of_regions_in_both_data_sources)\n",
    "    print(len(set_of_regions_in_both_data_sources))\n",
    "    prediction_from_in_flow = {}\n",
    "    prediction_from_out_flow = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for key in in_road_traffic_count_dictionary.keys():\n",
    "        destination = key[0]\n",
    "        if destination in set_of_regions_in_both_data_sources:\n",
    "            vehicle_count_sum = sum(in_road_traffic_count_dictionary[key])\n",
    "            passenger_sum = vehicle_count_sum * passenger_per_car\n",
    "            hour = str(key[1])\n",
    "            day = str(key[2])\n",
    "            month = str(key[3])\n",
    "            year = str(key[4])\n",
    "            if hour in {\"0\", \"6\"}: #morning\n",
    "                for origin in region_list:\n",
    "                    if origin != destination:\n",
    "                        frac = in_fractions_dictionary[origin, destination]\n",
    "                        result = passenger_sum * frac\n",
    "                        prediction_from_in_flow[(origin, destination, hour, day, month, year)] = result\n",
    "                        \n",
    "\n",
    "            elif hour in {\"12\", \"18\"}: #afternoon\n",
    "                for origin in region_list:\n",
    "                    if origin != destination:\n",
    "                        frac = out_fractions_dictionary[destination, origin]\n",
    "                        result = passenger_sum * frac\n",
    "                        prediction_from_in_flow[(destination, origin, hour, day, month, year)] = result\n",
    "\n",
    "            \n",
    "    \n",
    "    for key in out_road_traffic_count_dictionary.keys(): \n",
    "        origin = key[0]\n",
    "        if origin in set_of_regions_in_both_data_sources:\n",
    "            vehicle_count_sum = sum(out_road_traffic_count_dictionary[key])\n",
    "            passenger_sum = vehicle_count_sum * passenger_per_car\n",
    "            hour = str(key[1])\n",
    "            day = str(key[2])\n",
    "            month = str(key[3])\n",
    "            year = str(key[4])\n",
    "            if hour in {\"0\", \"6\"}: #morning\n",
    "                for destination in region_list:\n",
    "                    if origin != destination:\n",
    "                        frac = out_fractions_dictionary[origin, destination]\n",
    "                        result = passenger_sum * frac\n",
    "                        prediction_from_out_flow[(origin, destination, hour, day, month, year)] = result\n",
    "            elif hour in {\"12\", \"18\"}: #afternoon\n",
    "                for destination in region_list:\n",
    "                    if origin != destination:\n",
    "                        frac = in_fractions_dictionary[destination, origin]\n",
    "                        result = passenger_sum * frac\n",
    "                        prediction_from_out_flow[(destination, origin, hour, day, month, year)] = result\n",
    "    \n",
    "    \n",
    "    mutual_in_and_out_prediction_key_set = set(prediction_from_out_flow.keys()).intersection(set(prediction_from_in_flow))\n",
    "    print(\"mutual keys\")\n",
    "    print(len(mutual_in_and_out_prediction_key_set))\n",
    "    combined_prediction = {}\n",
    "    for key in prediction_from_in_flow.keys():\n",
    "        combined_prediction[key] = (prediction_from_in_flow[key] + prediction_from_out_flow[key])/2.0\n",
    "    if path_to_save_results:\n",
    "        print(\"printing\")\n",
    "        #write the files in a simillar formatting to the operator data\n",
    "        with open(path_to_save_results, 'w') as results_file:\n",
    "            header_list = [\"flow_estimate\", \"timestamp\", \"date\", \"day_of_the_week\", \"hour\", \"origin_hcd\", \"destination_hcd\"]\n",
    "            header_to_write = \",\".join(header_list)+\"\\n\"\n",
    "            results_file.write(header_to_write)\n",
    "            for key in combined_prediction.keys():\n",
    "                hour = str(key[2])\n",
    "                day = str(key[3])\n",
    "                month = str(key[4])\n",
    "                year = str(key[5])\n",
    "                date_string = year+\"-\"+month+\"-\"+day\n",
    "                weekday = date_to_weekday(day, month, year)\n",
    "                datetime_string = year+\"-\"+month+\"-\"+day+\" \"+hour+\":00\"\n",
    "                list_to_write = [str(combined_prediction[key]), datetime_string, date_string, weekday, hour, key[0], key[1]]\n",
    "                line_to_write = \",\".join(list_to_write)+\"\\n\"\n",
    "                results_file.write(line_to_write)         \n",
    "    return combined_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radiation Model\n",
    "read the estimates of static radiation model in the level of hospital care districts from the saved pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "radiation_static_commuters_book = {}\n",
    "hospital_radiation_path = (path_for_saving_estimations+\"/radiation_model_commuters_hcd.pickle\")\n",
    "with open(hospital_radiation_path,'rb') as f:\n",
    "     radiation_static_commuters_book = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function calculates the outgoing and incomming fractions for orign-destination pairs when the commute inside regions is excluded (since population units when calculating radiation model results are municipalities a large fraction of predicted commute is inside hospital care districts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_excluded_fractions_from_static_model_results(static_results_book):\n",
    "    #the fractions excluding inside-region mobility\n",
    "    incomming_frac_book = {}\n",
    "    outgoing_frac_book = {}\n",
    "    sum_incomming = {}\n",
    "    sum_outgoing = {}\n",
    "    for (origin, destination) in static_results_book.keys():\n",
    "        if origin != destination:\n",
    "            value = static_results_book[(origin, destination)]\n",
    "            if origin not in sum_outgoing.keys():\n",
    "                sum_outgoing[origin] = 0\n",
    "            sum_outgoing[origin] += value\n",
    "            if destination not in sum_incomming.keys():\n",
    "                sum_incomming[destination] = 0\n",
    "            sum_incomming[destination] += value\n",
    "    for (origin, destination) in static_results_book.keys():\n",
    "        if origin != destination:\n",
    "            outgoing_frac_book[(origin, destination)] = static_results_book[(origin, destination)]/sum_outgoing[origin]\n",
    "            incomming_frac_book[(origin, destination)] = static_results_book[(origin, destination)]/sum_incomming[destination]\n",
    "    return(outgoing_frac_book,incomming_frac_book)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "radiation_out_excluding_frac, radiation_in_excluding_frac = calculate_excluded_fractions_from_static_model_results(radiation_static_commuters_book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gravity Models\n",
    "Gravity model with commuters' data from statistics Finland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (deserialize)\n",
    "gravity_static_commuters_path = path_for_saving_estimations + \"/gravity_model_commuters_hcd.pickle\"\n",
    "gravity_static_commuters_book = {}\n",
    "with open(gravity_static_commuters_path, 'rb') as handle:\n",
    "    gravity_static_commuters_book = pickle.load(handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gravity_static_commuters_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the estimations of one the gravity models is missing some of the od values (wher the model predicts near zero values)\n",
    "for key in radiation_static_commuters_book.keys():\n",
    "    if key not in gravity_static_commuters_book.keys():\n",
    "        gravity_static_commuters_book[key] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gravity_static_commuters_book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine road data with gravity model to get dynamic estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravity_commuters_out_excluding_frac, gravity_commuters_in_excluding_frac = calculate_excluded_fractions_from_static_model_results(gravity_static_commuters_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Länsi-Pohja Hospital District', 'Central Ostrobothnia Hospital District', 'Helsinki and Uusimaa Hospital District', 'Kainuu Hospital District', 'Satakunta Hospital District', 'Vaasa Hospital District', 'Lappi Hospital District', 'Päijät-Häme Hospital District', 'Kymenlaakso Hospital District', 'South Karelia Hospital District', 'Southwest Finland Hospital District', 'North Savo Hospital District', 'Itä-Savo Hospital District', 'North Ostrobothnia Hospital District', 'Central Finland Hospital District', 'South Ostrobothnia Hospital District', 'Kanta-Häme Hospital District', 'Pirkanmaa Hospital District', 'South Savo Hospital District', 'North Karelia Hospital District'}\n",
      "{'Länsi-Pohja Hospital District', 'Central Ostrobothnia Hospital District', 'Helsinki and Uusimaa Hospital District', 'Kainuu Hospital District', 'Satakunta Hospital District', 'Vaasa Hospital District', 'Lappi Hospital District', 'Päijät-Häme Hospital District', 'Kymenlaakso Hospital District', 'South Karelia Hospital District', 'Southwest Finland Hospital District', 'North Savo Hospital District', 'Itä-Savo Hospital District', 'North Ostrobothnia Hospital District', 'Central Finland Hospital District', 'South Ostrobothnia Hospital District', 'Kanta-Häme Hospital District', 'Pirkanmaa Hospital District', 'South Savo Hospital District', 'North Karelia Hospital District'}\n",
      "20\n",
      "mutual keys\n",
      "362520\n"
     ]
    }
   ],
   "source": [
    "passengers_per_car = 1\n",
    "dynamic_commuters_gravity_estimation_book = combine_road_traffic_with_static_fractions(hcd_traffic_in_degree, gravity_commuters_in_excluding_frac, hcd_traffic_out_degree, gravity_commuters_out_excluding_frac, passengers_per_car, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle estimations\n",
    "save_path = path_for_saving_estimations + \"/dynamic_road_commuters_gravity_book.pkl\"\n",
    "with open(save_path, 'wb') as handle:\n",
    "    pickle.dump(dynamic_commuters_gravity_estimation_book, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine road data with radiation model to get dynamic estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Länsi-Pohja Hospital District', 'Central Ostrobothnia Hospital District', 'Helsinki and Uusimaa Hospital District', 'Kainuu Hospital District', 'Satakunta Hospital District', 'Vaasa Hospital District', 'Lappi Hospital District', 'Päijät-Häme Hospital District', 'Kymenlaakso Hospital District', 'South Karelia Hospital District', 'Southwest Finland Hospital District', 'North Savo Hospital District', 'Itä-Savo Hospital District', 'North Ostrobothnia Hospital District', 'Central Finland Hospital District', 'South Ostrobothnia Hospital District', 'Kanta-Häme Hospital District', 'Pirkanmaa Hospital District', 'South Savo Hospital District', 'North Karelia Hospital District'}\n",
      "{'Länsi-Pohja Hospital District', 'Central Ostrobothnia Hospital District', 'Helsinki and Uusimaa Hospital District', 'Kainuu Hospital District', 'Satakunta Hospital District', 'Vaasa Hospital District', 'Lappi Hospital District', 'Päijät-Häme Hospital District', 'Kymenlaakso Hospital District', 'South Karelia Hospital District', 'Southwest Finland Hospital District', 'North Savo Hospital District', 'Itä-Savo Hospital District', 'North Ostrobothnia Hospital District', 'Central Finland Hospital District', 'South Ostrobothnia Hospital District', 'Kanta-Häme Hospital District', 'Pirkanmaa Hospital District', 'South Savo Hospital District', 'North Karelia Hospital District'}\n",
      "20\n",
      "mutual keys\n",
      "362520\n"
     ]
    }
   ],
   "source": [
    "passengers_per_car = 1\n",
    "dynamic_commuters_radiation_estimation_book = combine_road_traffic_with_static_fractions(hcd_traffic_in_degree, radiation_in_excluding_frac, hcd_traffic_out_degree, radiation_out_excluding_frac, passengers_per_car, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle estimations\n",
    "save_path = path_for_saving_estimations + \"/radiation_road_estimations.pkl\"\n",
    "with open(save_path, 'wb') as handle:\n",
    "    pickle.dump(dynamic_commuters_radiation_estimation_book, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/cs/networks/heydars1/dynamic-mobility/results/radiation_road_estimations.pkl\n"
     ]
    }
   ],
   "source": [
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3/anaconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
