{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the repository we are working in so later is easier to read/write files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/cs/networks/heydars1/dynamic-mobility/data\n"
     ]
    }
   ],
   "source": [
    "dir_path = os.path.abspath('')\n",
    "data_path = os.path.join(dir_path, 'data')\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_to_h_data_converted_format = os.path.join(data_path, 'kunta_utf-8_trimmed_include_both_swedish_and_Finnish.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will read the file `kunta_utf-8_trimmed_include_both_swedish_and_Finnish.csv` and generate 2 dictionaries. The first one will map the hospital care districts (HCD) to the municipalities. The second one will do the inverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_municiplaities_to_hospital_district_areas(data_path):\n",
    "    line_counter = 0\n",
    "    m_to_h = {}\n",
    "    h_to_m = {}\n",
    "    with open(data_path, 'r') as data:\n",
    "        for line in data:\n",
    "            line_counter += 1\n",
    "            ### the enteries start from 6th line\n",
    "            if line_counter > 5:\n",
    "                fields = line.strip().split(';')\n",
    "                municipality_string = fields[1]\n",
    "                #print(municipality_string)\n",
    "                municipality = municipality_string[1:-1]\n",
    "                #print(municipality)\n",
    "                hospital_string = fields[3]\n",
    "                hospital_district = hospital_string[1:-1]\n",
    "                #print(hospital_district)\n",
    "                m_to_h[municipality] = hospital_district\n",
    "                if hospital_district not in h_to_m.keys():\n",
    "                    h_to_m[hospital_district] = set()\n",
    "                h_to_m[hospital_district].add(municipality)\n",
    "    return h_to_m, m_to_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Brändö',\n",
       " 'Eckerö',\n",
       " 'Finström',\n",
       " 'Föglö',\n",
       " 'Geta',\n",
       " 'Hammarland',\n",
       " 'Jomala',\n",
       " 'Kumlinge',\n",
       " 'Kökar',\n",
       " 'Lemland',\n",
       " 'Lumparland',\n",
       " 'Maarianhamina - Mariehamn',\n",
       " 'Mariehamn',\n",
       " 'Saltvik',\n",
       " 'Sottunga',\n",
       " 'Sund',\n",
       " 'Vårdö'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_to_m, m_to_h = map_municiplaities_to_hospital_district_areas(m_to_h_data_converted_format)\n",
    "# print(h_to_m)\n",
    "# print(m_to_h)\n",
    "h_to_m['Åland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_commutters_data(data_path, text_encoding_format, number_of_header_lines,\n",
    "                        number_of_coloumns, measure_coloumn):\n",
    "    measure_book = {}\n",
    "    line_counter = 0\n",
    "    aland_set = {'Brändö','Eckerö','Finström','Föglö','Geta','Hammarland','Jomala','Kumlinge','Kökar','Lemland','Lumparland','Maarianhamina - Mariehamn','Mariehamn','Saltvik','Sottunga','Sund','Vårdö'}\n",
    "    with open(data_path, 'r', encoding=text_encoding_format) as data:\n",
    "        for line in data:\n",
    "            line_counter += 1\n",
    "            if line_counter > number_of_header_lines:\n",
    "                #from IPython.core.debugger import Pdb\n",
    "                #Pdb().set_trace()\n",
    "                fields = line.strip().split(';')\n",
    "                assert len(fields) == number_of_coloumns\n",
    "                city = fields[0]\n",
    "                if city not in aland_set:\n",
    "                    measure = int(fields[measure_coloumn])\n",
    "                    measure_book[city] = measure\n",
    "    return measure_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def calculate_distance_matrix(coordinates_book):\n",
    "    ### aim: calculating the distance matrix between all the population centers \n",
    "    ### input: coordinates_book\n",
    "    ###        a dictionary with population center names as keys and their (x,y) coordinates as values\n",
    "    ### output: distance_book\n",
    "    ###         a dictionary with (pop_cent_i, pop_cent_j) as keys and their distance as value\n",
    "    distance_book = {}\n",
    "    pairs = list(itertools.combinations(coordinates_book.keys(), 2))\n",
    "    for city_pair in pairs:\n",
    "        i = city_pair[0]\n",
    "        j = city_pair[1]\n",
    "        x_i, y_i = coordinates_book[i][0], coordinates_book[i][1]\n",
    "        x_j, y_j = coordinates_book[j][0], coordinates_book[j][1]\n",
    "        d = np.sqrt((x_i - x_j)**2+(y_i - y_j)**2)\n",
    "        distance_book[(i,j)] = d\n",
    "        distance_book[(j,i)] = d\n",
    "    return distance_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def population_sum_between_origin_and_destination(population_book, distance_book, origin, destination):\n",
    "    d_ij = distance_book[(origin,destination)]\n",
    "    ID_set = set(population_book.keys()).difference({origin, destination})\n",
    "    sum_ij = 0\n",
    "    for city in ID_set:\n",
    "        #from IPython.core.debugger import Pdb\n",
    "        #Pdb().set_trace()\n",
    "        if distance_book[(city, origin)] < d_ij:\n",
    "            sum_ij += population_book[city]\n",
    "    return sum_ij\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_from_i_to_j(normalization_factor, workers_living_in_i, jobs_in_i, jobs_in_j, s_ij):\n",
    "    p_ij = (normalization_factor*jobs_in_i*jobs_in_j)/((jobs_in_i+s_ij)*(jobs_in_i+jobs_in_j+s_ij)) \n",
    "    flow_ij = workers_living_in_i*p_ij\n",
    "    return flow_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_workers_who_both_live_and_work_in_i (workers_living_in_i, outcommuters_of_i):\n",
    "    n_ii = workers_living_in_i - out_commuters_from_i\n",
    "    return n_ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.abspath('')\n",
    "#data_path = os.path.join(dir_path, 'data')\n",
    "#print(data_path)\n",
    "\n",
    "#figuring out the path to the location where our script is saved\n",
    "#import inspect, os.path\n",
    "#filename = inspect.getframeinfo(inspect.currentframe()).filename\n",
    "#dir_path = os.path.dirname(os.path.abspath(filename))\n",
    "    \n",
    "### Reading the data from statistics Finland on number of out-commuters of each municipality, \"ni+\"\n",
    "out_commutters_data_path = dir_path + \"/data/population_data/out_degree_cleaned.csv\"\n",
    "#out_commutters_data_path = \"/m/cs/scratch/networks/heydars1/population_flow/data/population_data/out_degree_cleaned.csv\"\n",
    "out_commuters_book = read_commutters_data(out_commutters_data_path, 'iso-8859-1', 4, 12, 3)\n",
    "    \n",
    "### Reading the data from statistics Finland on total number of workers living in each municipality, \"ni:\"\n",
    "num_workers_by_home_municipality_data_path = \"/m/cs/scratch/networks/heydars1/population_flow/data/population_data/num_employed_by_home_municipality_cleaned.csv\"\n",
    "workers_by_home_municipality_book = read_commutters_data(num_workers_by_home_municipality_data_path, 'iso-8859-1', 3, 11, 2)\n",
    "    \n",
    "### Reading the data from statistics Finland on total number of jobs located in each municipality, \"n:i\"\n",
    "num_jobs_in_each_muncipality_data_path = \"/m/cs/scratch/networks/heydars1/population_flow/data/population_data/num_jobs_in_each_municipality_cleaned.csv\"\n",
    "jobs_in_each_municipality_book = read_commutters_data(num_jobs_in_each_muncipality_data_path, 'iso-8859-1', 3, 11, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(workers_by_home_municipality_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\n",
      "294\n"
     ]
    }
   ],
   "source": [
    "coordinates_book = pickle.load( open( dir_path + \"/data/population_data/centre_of_populations_of_municiplaities.pkl\", \"rb\" ) )\n",
    "print(len(coordinates_book))\n",
    "aland_set = {'Brändö','Eckerö','Finström','Föglö','Geta','Hammarland','Jomala','Kumlinge','Kökar','Lemland','Lumparland','Maarianhamina - Mariehamn','Mariehamn','Saltvik','Sottunga','Sund','Vårdö'}\n",
    "entries_to_remove = aland_set\n",
    "for m in entries_to_remove:\n",
    "    coordinates_book.pop(m, None)\n",
    "print(len(coordinates_book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Municipality Valtimo has been merged with Nurmes recently. Even if our both population data and commuter data are from 2017, Valimo exist in commuting data but not in population data. For convenience, we remove Valtimo from our dictionaries.\n",
    "del out_commuters_book['Valtimo']\n",
    "del jobs_in_each_municipality_book['Valtimo']\n",
    "del workers_by_home_municipality_book['Valtimo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/cs/networks/heydars1/dynamic-mobility/results/radiation_model_number_of_commuters_as_prior.csv\n"
     ]
    }
   ],
   "source": [
    "#calculating distance between all pairs of population centers\n",
    "distance_book = calculate_distance_matrix(coordinates_book)\n",
    "total_num_jobs_in_country = sum(list(jobs_in_each_municipality_book.values()))\n",
    "\n",
    "##printing the header of .csv file\n",
    "seperator = ','\n",
    "header_couloumns = [\"origin\",\"destination\",\"workers living in i\",\"jobs in i\",\"jobs in j\",\"jobs in between\",\"estimated_population_flow\"]\n",
    "path_for_saving_results = dir_path + \"/results/radiation_model_number_of_commuters_as_prior.csv\"\n",
    "print(path_for_saving_results)\n",
    "with open(path_for_saving_results, 'w') as outfile:\n",
    "    outfile.write(seperator.join(header_couloumns) + \"\\n\")\n",
    "        \n",
    "    for city_pair in distance_book.keys():\n",
    "        i = city_pair[0]\n",
    "        j = city_pair[1]\n",
    "        d = distance_book[city_pair]\n",
    "        ### Caculating s_ij, sum of population of all the regions which their distance to region i is less than d_ij\n",
    "        s_ij = population_sum_between_origin_and_destination(jobs_in_each_municipality_book, distance_book, i, j)\n",
    "        jobs_in_i = jobs_in_each_municipality_book[i]\n",
    "        jobs_in_j = jobs_in_each_municipality_book[j]\n",
    "        out_commuters_from_i = out_commuters_book[i]\n",
    "        workers_living_in_i = workers_by_home_municipality_book[i]\n",
    "        ### Calculating daily commuting flow from i to j based on the radiation model (number of people who live in region i but work in region j )\n",
    "        normalization_factor = out_commuters_from_i/((workers_living_in_i)*(1-jobs_in_i/total_num_jobs_in_country))\n",
    "        assert normalization_factor <= 1\n",
    "        flow_ij = flow_from_i_to_j(normalization_factor, workers_living_in_i, jobs_in_i, jobs_in_j, s_ij) #estimated traffic flow from i to j\n",
    "        #printing the results\n",
    "        value_list = [city_pair[0], city_pair[1], str(workers_living_in_i), str(jobs_in_i), str(jobs_in_j), str(s_ij), str(flow_ij)]\n",
    "        line_to_write = seperator.join(value_list) + \"\\n\"\n",
    "        #print(seperator.join(value_list))\n",
    "        outfile.write(line_to_write)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3/anaconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
